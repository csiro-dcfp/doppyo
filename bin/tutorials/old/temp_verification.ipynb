{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial demonstrating verification of v1 1000hPa temp against jra55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pyLatte package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatte import utils\n",
    "from pylatte import skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently, the following packages are required to load the data - this process will be replaced by the CAFE cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "# import warnings    \n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "# Jupyter specific -----\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note about the methodology of pyLatte\n",
    "The pyLatte package is constructed around the xarray Python package. This is particularly useful for verification computations which require large numbers of samples (different model runs) to converge. \n",
    "\n",
    "The approach here is to generate very large xarray objects that reference all data required for the verification, but do not store the data in memory. Operations are performed on these xarray objects out-of-memory. When it is necessary to perform a compute (e.g. to produce a plot), this is distributed over multiple processors using the dask Python package.\n",
    "\n",
    "### Important:\n",
    "To simplify comparison between different data-sets, the functions in pyLatte assume that the datetime value given for a particular interval corresponds to the START of that interval. E.g., for monthly frequency data `time = ['2002-01-01, 2002-02-01, 2002-03-01,...]`. The function `utils.trunc_time()` can be used to truncate timedeltas to the start value of a period corresponding to a provided frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise dask (currently not working on vm31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import distributed\n",
    "# client = distributed.Client(local_dir='/tmp/squ027-dask-worker-space', n_workers=4)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct xarray objects for forecasts and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The CAFE cookbook should replace many of these code blocks)\n",
    "\n",
    "At the moment, the user must specify the frequency of the data they are choosing to load. This is necessary for keeping track of the correct units of lead time when time information is formatted as lead time/initial date rather than datatime. `pandas` provides a function `pandas.infer_freq()` for determining the most likely frequency from a datetime array. However, this function is fairly limited because of the difficulties in defining '1 month'. For example, `pandas.infer_freq()` is unable to determine the frequency of the ocean_month data, which has time values, e.g., `time = ['2003-01-16T12:00:00', '2003-02-15T00:00:00', '2003-03-16T12:00:00'...]`. Thus, for now, we leave it up to the user to correctly input the data frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of forecast data -----\n",
    "fcst_folder = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/'\n",
    "fcst_filename = 'atmos_daily*'\n",
    "fcst_variable = 'temp'\n",
    "fcst_freq = 'D' # e.g. 'A', '3M', 'M', '7D', '6H'...\n",
    "\n",
    "# Location of observation data -----\n",
    "obsv_folder = '/OSM/CBR/OA_DCFP/data/observations/jra55/isobaric/011_tmp/cat/'\n",
    "obsv_filename = 'jra.55.tmp.1000.1958010100_2016123118.nc'\n",
    "obsv_variable = 'TMP_GDS0_ISBL'\n",
    "obsv_freq = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dates to include (takes approximately 1 min 30 sec per date) -----\n",
    "init_dates = pd.date_range('2003-1','2003-12' , freq='1MS')\n",
    "\n",
    "# Ensembles to include -----\n",
    "ensembles = range(1,12)\n",
    "\n",
    "# Forecast length -----\n",
    "FCST_LENGTH = 2 # years\n",
    "# lead_times = utils.get_lead_times(FCST_LENGTH, resample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling details -----\n",
    "resample_freq = 'M'\n",
    "resample_method = 'mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct forecasts xarray object\n",
    "Note, dask has a known bug that manifests when trying to concatentate data containing timedelta64 arrays (see https://github.com/pydata/xarray/issues/1952 for further details). For example, try to concatenate the following two Datasets:\n",
    "\n",
    "`In : path = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/yr2002/mn7/'`\n",
    "\n",
    "`In : ens5 = xr.open_mfdataset(path + 'OUTPUT.5/atmos_daily*.nc', autoclose=True)`\n",
    "\n",
    "`In : ens6 = xr.open_mfdataset(path + 'OUTPUT.6/atmos_daily*.nc', autoclose=True)`\n",
    "\n",
    "`In : xr.concat([ens5, ens6],'ensemble')`\n",
    "\n",
    "`Out : TypeError: invalid type promotion`\n",
    "\n",
    "The error here is actually caused by the variables `average_DT` and `time_bounds`, which are timedelta64 arrays. However, I still do not fully unstand the bug: concatenation of `ens4` and `ens5`, for example, works fine, even though `ens4` also contains the timedelta64 variables `average_DT` and `time_bounds`. Regardless, because of this bug, it is not possible currently to create an xarray Dataset object containing all model variables. Instead, only the variable of interest (i.e. `fcst_variable` and `obsv_variable`) are retained in the concatenated xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0b1f64bc524cd0986a1c2456f9a1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0, description='Loading...', max=132.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate progress bar -----\n",
    "f = FloatProgress(min=0, max=len(init_dates)*len(ensembles), description='Loading...') \n",
    "display(f)\n",
    "\n",
    "# Loop over initial dates -----\n",
    "fcst_list = []\n",
    "for init_date in init_dates:\n",
    "    year = init_date.year\n",
    "    month = init_date.month\n",
    "    \n",
    "    # Loop over ensembles -----\n",
    "    ens_list = []\n",
    "    for ensemble in ensembles:\n",
    "        # Signal to increment the progress bar -----\n",
    "        f.value += 1 \n",
    "        \n",
    "        # Stack ensembles into a list -----\n",
    "        path = fcst_folder + '/yr' + str(year) + '/mn' + str(month) + \\\n",
    "               '/OUTPUT.' + str(ensemble) + '/' + fcst_filename + '.nc'\n",
    "        ens_list.append(xr.open_mfdataset(path, autoclose=True)[fcst_variable])\n",
    "        \n",
    "    # Concatenate ensembles -----\n",
    "    ens_object = xr.concat(ens_list, dim='ensemble')\n",
    "    ens_object['ensemble'] = ensembles\n",
    "    \n",
    "    # Stack concatenated ensembles into a list for each initial date -----\n",
    "    fcst_list.append(ens_object)\n",
    "\n",
    "# Keep track of the lead time for each initialization -----\n",
    "n_lead_time = [len(x.time) for x in fcst_list]\n",
    "\n",
    "# Keep track of initial dates truncated to freqeuncy of runs -----\n",
    "start_dates = np.array([utils.trunc_time(x.time.values[0], fcst_freq) for x in fcst_list])\n",
    "\n",
    "# Concatenate initial dates -----\n",
    "da_fcst = xr.concat(fcst_list, dim='init_date').rename({'time' : 'lead_time'})\n",
    "da_fcst['init_date'] = start_dates\n",
    "da_fcst['lead_time'] = range(len(da_fcst.lead_time))\n",
    "da_fcst['lead_time'].attrs['units'] = fcst_freq\n",
    "\n",
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "da_fcst = utils.prune(da_fcst.chunk(chunks={'ensemble' : len(da_fcst.ensemble), \n",
    "                                            'lead_time' : len(da_fcst.lead_time)}).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate the forecast lead times at 2 years\n",
    "The January and July forecasts are run for 5 years rather than 2 years. The xarray concatenation above can deal with this, but fills the shorter forecasts with nans for lead times longer than 2 years. Let's get rid of some of these nans by truncating the forecasts at the lead time corresponding to the longest 2 year forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_increments = FCST_LENGTH * 366\n",
    "n_trunc = max([i for i in n_lead_time if i <= max_increments])\n",
    "da_fcst = da_fcst.isel(lead_time=range(n_trunc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct observations xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aef0ed79253426ab348e92ffed057a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0, description='Loading...', max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate progress bar -----\n",
    "f = FloatProgress(min=0, max=1, description='Loading...') \n",
    "display(f)\n",
    "\n",
    "# JRA temperature fields are only save in a time-concatenated form -----\n",
    "path = obsv_folder + obsv_filename\n",
    "da_obsv = (xr.open_mfdataset(path, autoclose=True)[obsv_variable]).rename(fcst_variable) \\\n",
    "                                                               .rename({'initial_time0_hours' : 'time',\n",
    "                                                                        'g0_lon_3' : 'lon',\n",
    "                                                                        'g0_lat_2' : 'lat'})\n",
    "    \n",
    "# Truncate jra frequency to forecast frequency -----\n",
    "da_obsv['time'] = utils.trunc_time(da_obsv['time'].values, da_fcst.lead_time.attrs['units'])\n",
    "\n",
    "# Stack by initial date to match forecast structure -----\n",
    "da_obsv = utils.stack_by_init_date(da_obsv,da_fcst.init_date.values,n_trunc)\n",
    "f.value += 1\n",
    "\n",
    "# Average over forecast dimension if it is exists -----\n",
    "if 'forecast_time1' in da_obsv.coords:\n",
    "    da_obsv[obsv_variable] = da_obsv[obsv_variable].mean(dim='forecast_time1')\n",
    "\n",
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "da_obsv = utils.prune(da_obsv.chunk(chunks={'init_date' : len(da_obsv.init_date)}).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample forecast and observations to desired frequency\n",
    "The data are currently stored in lead time/initial date format. The easiest and fastest way to perform the resampling is to to use `xr.resample()` which requires that time information be in datetime format (note, this process in wrapped in the pyLatte `utils.resample()` function). Thus it is necessary to first convert from the lead time/initial date format to a datetime format, then resample the data, then convert back to the lead time/initial date format. The `utils.leadtime_to_datetime()` and `utils.datetime_to_leadtime()` functions enable these types of operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_delta(date_in, delta, trunc_to_start=False):\n",
    "    \"\"\" Increments provided datetime64 array by delta months \"\"\"\n",
    "    \n",
    "    date_mod = pd.Timestamp(date_in)\n",
    "    \n",
    "    m, y = (date_mod.month + delta) % 12, date_mod.year + ((date_mod.month) + delta - 1) // 12\n",
    "    if not m: m = 12\n",
    "    d = min(date_mod.day, [31,\n",
    "        29 if y % 4 == 0 and not y % 400 == 0 else 28,31,30,31,30,31,31,30,31,30,31][m - 1])\n",
    "    \n",
    "    if trunc_to_start:\n",
    "        date_out = utils.trunc_time(np.datetime64(date_mod.replace(day=d,month=m, year=y)),'M')\n",
    "    else:\n",
    "        date_out = np.datetime64(date_mod.replace(day=d,month=m, year=y))\n",
    "    return date_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_delta(date_in, delta, trunc_to_start=False):\n",
    "    \"\"\" Increments provided datetime64 array by delta years \"\"\"\n",
    "    \n",
    "    date_mod = month_delta(date_in, 12 * delta)\n",
    "    \n",
    "    if trunc_to_start:\n",
    "        date_out = utils.trunc_time(date_mod,'Y')\n",
    "    else: date_out = date_mod\n",
    "        \n",
    "    return date_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leadtime_to_datetime(data, lead_time_name='lead_time', init_date_name='init_date'):\n",
    "    \"\"\" Converts time information from lead time/initial date dimension pair to single datetime dimension \"\"\"\n",
    "    \n",
    "    init_date = data[init_date_name].values[0]\n",
    "    lead_times = list(map(int,data[lead_time_name].values))\n",
    "    freq = data[lead_time_name].attrs['units']\n",
    "\n",
    "    # Deal with special cases of monthly and yearly frequencies -----\n",
    "    if 'M' in freq:\n",
    "        # Check if multiple months are specified\n",
    "        if len(freq) > 1:\n",
    "            num_months = int(freq.replace(\"M\", \"\"))\n",
    "        else: num_months = 1\n",
    "            \n",
    "        datetimes = np.array([month_delta(init_date, num_months * ix) for ix in lead_times])\n",
    "    elif 'A' in freq:\n",
    "        # Check if multiple months are specified\n",
    "        if len(freq) > 1:\n",
    "            num_years = int(freq.replace(\"A\", \"\"))\n",
    "        else: num_years = 1\n",
    "            \n",
    "        datetimes = np.array([year_delta(init_date, num_years * ix) for ix in lead_times])\n",
    "    else:\n",
    "        datetimes = (pd.date_range(init_date, periods=len(lead_times), freq=freq)).values\n",
    "    \n",
    "    data = data.drop(init_date_name)\n",
    "    data = data.rename({lead_time_name : 'time'})\n",
    "    data['time'] = datetimes\n",
    "    \n",
    "    return utils.prune(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2003-01-01T00:00:00.000000000' '2003-01-03T00:00:00.000000000'\n",
      " '2003-01-05T00:00:00.000000000' '2003-01-07T00:00:00.000000000'\n",
      " '2003-01-09T00:00:00.000000000' '2003-01-11T00:00:00.000000000'\n",
      " '2003-01-13T00:00:00.000000000' '2003-01-15T00:00:00.000000000'\n",
      " '2003-01-17T00:00:00.000000000' '2003-01-19T00:00:00.000000000'\n",
      " '2003-01-21T00:00:00.000000000' '2003-01-23T00:00:00.000000000'\n",
      " '2003-01-25T00:00:00.000000000' '2003-01-27T00:00:00.000000000'\n",
      " '2003-01-29T00:00:00.000000000' '2003-01-31T00:00:00.000000000'\n",
      " '2003-02-02T00:00:00.000000000' '2003-02-04T00:00:00.000000000'\n",
      " '2003-02-06T00:00:00.000000000' '2003-02-08T00:00:00.000000000'\n",
      " '2003-02-10T00:00:00.000000000' '2003-02-12T00:00:00.000000000'\n",
      " '2003-02-14T00:00:00.000000000' '2003-02-16T00:00:00.000000000'\n",
      " '2003-02-18T00:00:00.000000000' '2003-02-20T00:00:00.000000000'\n",
      " '2003-02-22T00:00:00.000000000' '2003-02-24T00:00:00.000000000'\n",
      " '2003-02-26T00:00:00.000000000' '2003-02-28T00:00:00.000000000'\n",
      " '2003-03-02T00:00:00.000000000' '2003-03-04T00:00:00.000000000'\n",
      " '2003-03-06T00:00:00.000000000' '2003-03-08T00:00:00.000000000'\n",
      " '2003-03-10T00:00:00.000000000' '2003-03-12T00:00:00.000000000'\n",
      " '2003-03-14T00:00:00.000000000' '2003-03-16T00:00:00.000000000'\n",
      " '2003-03-18T00:00:00.000000000' '2003-03-20T00:00:00.000000000'\n",
      " '2003-03-22T00:00:00.000000000' '2003-03-24T00:00:00.000000000'\n",
      " '2003-03-26T00:00:00.000000000' '2003-03-28T00:00:00.000000000'\n",
      " '2003-03-30T00:00:00.000000000' '2003-04-01T00:00:00.000000000'\n",
      " '2003-04-03T00:00:00.000000000' '2003-04-05T00:00:00.000000000'\n",
      " '2003-04-07T00:00:00.000000000' '2003-04-09T00:00:00.000000000'\n",
      " '2003-04-11T00:00:00.000000000' '2003-04-13T00:00:00.000000000'\n",
      " '2003-04-15T00:00:00.000000000' '2003-04-17T00:00:00.000000000'\n",
      " '2003-04-19T00:00:00.000000000' '2003-04-21T00:00:00.000000000'\n",
      " '2003-04-23T00:00:00.000000000' '2003-04-25T00:00:00.000000000'\n",
      " '2003-04-27T00:00:00.000000000' '2003-04-29T00:00:00.000000000'\n",
      " '2003-05-01T00:00:00.000000000' '2003-05-03T00:00:00.000000000'\n",
      " '2003-05-05T00:00:00.000000000' '2003-05-07T00:00:00.000000000'\n",
      " '2003-05-09T00:00:00.000000000' '2003-05-11T00:00:00.000000000'\n",
      " '2003-05-13T00:00:00.000000000' '2003-05-15T00:00:00.000000000'\n",
      " '2003-05-17T00:00:00.000000000' '2003-05-19T00:00:00.000000000'\n",
      " '2003-05-21T00:00:00.000000000' '2003-05-23T00:00:00.000000000'\n",
      " '2003-05-25T00:00:00.000000000' '2003-05-27T00:00:00.000000000'\n",
      " '2003-05-29T00:00:00.000000000' '2003-05-31T00:00:00.000000000'\n",
      " '2003-06-02T00:00:00.000000000' '2003-06-04T00:00:00.000000000'\n",
      " '2003-06-06T00:00:00.000000000' '2003-06-08T00:00:00.000000000'\n",
      " '2003-06-10T00:00:00.000000000' '2003-06-12T00:00:00.000000000'\n",
      " '2003-06-14T00:00:00.000000000' '2003-06-16T00:00:00.000000000'\n",
      " '2003-06-18T00:00:00.000000000' '2003-06-20T00:00:00.000000000'\n",
      " '2003-06-22T00:00:00.000000000' '2003-06-24T00:00:00.000000000'\n",
      " '2003-06-26T00:00:00.000000000' '2003-06-28T00:00:00.000000000'\n",
      " '2003-06-30T00:00:00.000000000' '2003-07-02T00:00:00.000000000'\n",
      " '2003-07-04T00:00:00.000000000' '2003-07-06T00:00:00.000000000'\n",
      " '2003-07-08T00:00:00.000000000' '2003-07-10T00:00:00.000000000'\n",
      " '2003-07-12T00:00:00.000000000' '2003-07-14T00:00:00.000000000'\n",
      " '2003-07-16T00:00:00.000000000' '2003-07-18T00:00:00.000000000'\n",
      " '2003-07-20T00:00:00.000000000' '2003-07-22T00:00:00.000000000'\n",
      " '2003-07-24T00:00:00.000000000' '2003-07-26T00:00:00.000000000'\n",
      " '2003-07-28T00:00:00.000000000' '2003-07-30T00:00:00.000000000'\n",
      " '2003-08-01T00:00:00.000000000' '2003-08-03T00:00:00.000000000'\n",
      " '2003-08-05T00:00:00.000000000' '2003-08-07T00:00:00.000000000'\n",
      " '2003-08-09T00:00:00.000000000' '2003-08-11T00:00:00.000000000'\n",
      " '2003-08-13T00:00:00.000000000' '2003-08-15T00:00:00.000000000'\n",
      " '2003-08-17T00:00:00.000000000' '2003-08-19T00:00:00.000000000'\n",
      " '2003-08-21T00:00:00.000000000' '2003-08-23T00:00:00.000000000'\n",
      " '2003-08-25T00:00:00.000000000' '2003-08-27T00:00:00.000000000'\n",
      " '2003-08-29T00:00:00.000000000' '2003-08-31T00:00:00.000000000'\n",
      " '2003-09-02T00:00:00.000000000' '2003-09-04T00:00:00.000000000'\n",
      " '2003-09-06T00:00:00.000000000' '2003-09-08T00:00:00.000000000'\n",
      " '2003-09-10T00:00:00.000000000' '2003-09-12T00:00:00.000000000'\n",
      " '2003-09-14T00:00:00.000000000' '2003-09-16T00:00:00.000000000'\n",
      " '2003-09-18T00:00:00.000000000' '2003-09-20T00:00:00.000000000'\n",
      " '2003-09-22T00:00:00.000000000' '2003-09-24T00:00:00.000000000'\n",
      " '2003-09-26T00:00:00.000000000' '2003-09-28T00:00:00.000000000'\n",
      " '2003-09-30T00:00:00.000000000' '2003-10-02T00:00:00.000000000'\n",
      " '2003-10-04T00:00:00.000000000' '2003-10-06T00:00:00.000000000'\n",
      " '2003-10-08T00:00:00.000000000' '2003-10-10T00:00:00.000000000'\n",
      " '2003-10-12T00:00:00.000000000' '2003-10-14T00:00:00.000000000'\n",
      " '2003-10-16T00:00:00.000000000' '2003-10-18T00:00:00.000000000'\n",
      " '2003-10-20T00:00:00.000000000' '2003-10-22T00:00:00.000000000'\n",
      " '2003-10-24T00:00:00.000000000' '2003-10-26T00:00:00.000000000'\n",
      " '2003-10-28T00:00:00.000000000' '2003-10-30T00:00:00.000000000'\n",
      " '2003-11-01T00:00:00.000000000' '2003-11-03T00:00:00.000000000'\n",
      " '2003-11-05T00:00:00.000000000' '2003-11-07T00:00:00.000000000'\n",
      " '2003-11-09T00:00:00.000000000' '2003-11-11T00:00:00.000000000'\n",
      " '2003-11-13T00:00:00.000000000' '2003-11-15T00:00:00.000000000'\n",
      " '2003-11-17T00:00:00.000000000' '2003-11-19T00:00:00.000000000'\n",
      " '2003-11-21T00:00:00.000000000' '2003-11-23T00:00:00.000000000'\n",
      " '2003-11-25T00:00:00.000000000' '2003-11-27T00:00:00.000000000'\n",
      " '2003-11-29T00:00:00.000000000' '2003-12-01T00:00:00.000000000'\n",
      " '2003-12-03T00:00:00.000000000' '2003-12-05T00:00:00.000000000'\n",
      " '2003-12-07T00:00:00.000000000' '2003-12-09T00:00:00.000000000'\n",
      " '2003-12-11T00:00:00.000000000' '2003-12-13T00:00:00.000000000'\n",
      " '2003-12-15T00:00:00.000000000' '2003-12-17T00:00:00.000000000'\n",
      " '2003-12-19T00:00:00.000000000' '2003-12-21T00:00:00.000000000'\n",
      " '2003-12-23T00:00:00.000000000' '2003-12-25T00:00:00.000000000'\n",
      " '2003-12-27T00:00:00.000000000' '2003-12-29T00:00:00.000000000'\n",
      " '2003-12-31T00:00:00.000000000' '2004-01-02T00:00:00.000000000'\n",
      " '2004-01-04T00:00:00.000000000' '2004-01-06T00:00:00.000000000'\n",
      " '2004-01-08T00:00:00.000000000' '2004-01-10T00:00:00.000000000'\n",
      " '2004-01-12T00:00:00.000000000' '2004-01-14T00:00:00.000000000'\n",
      " '2004-01-16T00:00:00.000000000' '2004-01-18T00:00:00.000000000'\n",
      " '2004-01-20T00:00:00.000000000' '2004-01-22T00:00:00.000000000'\n",
      " '2004-01-24T00:00:00.000000000' '2004-01-26T00:00:00.000000000'\n",
      " '2004-01-28T00:00:00.000000000' '2004-01-30T00:00:00.000000000'\n",
      " '2004-02-01T00:00:00.000000000' '2004-02-03T00:00:00.000000000'\n",
      " '2004-02-05T00:00:00.000000000' '2004-02-07T00:00:00.000000000'\n",
      " '2004-02-09T00:00:00.000000000' '2004-02-11T00:00:00.000000000'\n",
      " '2004-02-13T00:00:00.000000000' '2004-02-15T00:00:00.000000000'\n",
      " '2004-02-17T00:00:00.000000000' '2004-02-19T00:00:00.000000000'\n",
      " '2004-02-21T00:00:00.000000000' '2004-02-23T00:00:00.000000000'\n",
      " '2004-02-25T00:00:00.000000000' '2004-02-27T00:00:00.000000000'\n",
      " '2004-02-29T00:00:00.000000000' '2004-03-02T00:00:00.000000000'\n",
      " '2004-03-04T00:00:00.000000000' '2004-03-06T00:00:00.000000000'\n",
      " '2004-03-08T00:00:00.000000000' '2004-03-10T00:00:00.000000000'\n",
      " '2004-03-12T00:00:00.000000000' '2004-03-14T00:00:00.000000000'\n",
      " '2004-03-16T00:00:00.000000000' '2004-03-18T00:00:00.000000000'\n",
      " '2004-03-20T00:00:00.000000000' '2004-03-22T00:00:00.000000000'\n",
      " '2004-03-24T00:00:00.000000000' '2004-03-26T00:00:00.000000000'\n",
      " '2004-03-28T00:00:00.000000000' '2004-03-30T00:00:00.000000000'\n",
      " '2004-04-01T00:00:00.000000000' '2004-04-03T00:00:00.000000000'\n",
      " '2004-04-05T00:00:00.000000000' '2004-04-07T00:00:00.000000000'\n",
      " '2004-04-09T00:00:00.000000000' '2004-04-11T00:00:00.000000000'\n",
      " '2004-04-13T00:00:00.000000000' '2004-04-15T00:00:00.000000000'\n",
      " '2004-04-17T00:00:00.000000000' '2004-04-19T00:00:00.000000000'\n",
      " '2004-04-21T00:00:00.000000000' '2004-04-23T00:00:00.000000000'\n",
      " '2004-04-25T00:00:00.000000000' '2004-04-27T00:00:00.000000000'\n",
      " '2004-04-29T00:00:00.000000000' '2004-05-01T00:00:00.000000000'\n",
      " '2004-05-03T00:00:00.000000000' '2004-05-05T00:00:00.000000000'\n",
      " '2004-05-07T00:00:00.000000000' '2004-05-09T00:00:00.000000000'\n",
      " '2004-05-11T00:00:00.000000000' '2004-05-13T00:00:00.000000000'\n",
      " '2004-05-15T00:00:00.000000000' '2004-05-17T00:00:00.000000000'\n",
      " '2004-05-19T00:00:00.000000000' '2004-05-21T00:00:00.000000000'\n",
      " '2004-05-23T00:00:00.000000000' '2004-05-25T00:00:00.000000000'\n",
      " '2004-05-27T00:00:00.000000000' '2004-05-29T00:00:00.000000000'\n",
      " '2004-05-31T00:00:00.000000000' '2004-06-02T00:00:00.000000000'\n",
      " '2004-06-04T00:00:00.000000000' '2004-06-06T00:00:00.000000000'\n",
      " '2004-06-08T00:00:00.000000000' '2004-06-10T00:00:00.000000000'\n",
      " '2004-06-12T00:00:00.000000000' '2004-06-14T00:00:00.000000000'\n",
      " '2004-06-16T00:00:00.000000000' '2004-06-18T00:00:00.000000000'\n",
      " '2004-06-20T00:00:00.000000000' '2004-06-22T00:00:00.000000000'\n",
      " '2004-06-24T00:00:00.000000000' '2004-06-26T00:00:00.000000000'\n",
      " '2004-06-28T00:00:00.000000000' '2004-06-30T00:00:00.000000000'\n",
      " '2004-07-02T00:00:00.000000000' '2004-07-04T00:00:00.000000000'\n",
      " '2004-07-06T00:00:00.000000000' '2004-07-08T00:00:00.000000000'\n",
      " '2004-07-10T00:00:00.000000000' '2004-07-12T00:00:00.000000000'\n",
      " '2004-07-14T00:00:00.000000000' '2004-07-16T00:00:00.000000000'\n",
      " '2004-07-18T00:00:00.000000000' '2004-07-20T00:00:00.000000000'\n",
      " '2004-07-22T00:00:00.000000000' '2004-07-24T00:00:00.000000000'\n",
      " '2004-07-26T00:00:00.000000000' '2004-07-28T00:00:00.000000000'\n",
      " '2004-07-30T00:00:00.000000000' '2004-08-01T00:00:00.000000000'\n",
      " '2004-08-03T00:00:00.000000000' '2004-08-05T00:00:00.000000000'\n",
      " '2004-08-07T00:00:00.000000000' '2004-08-09T00:00:00.000000000'\n",
      " '2004-08-11T00:00:00.000000000' '2004-08-13T00:00:00.000000000'\n",
      " '2004-08-15T00:00:00.000000000' '2004-08-17T00:00:00.000000000'\n",
      " '2004-08-19T00:00:00.000000000' '2004-08-21T00:00:00.000000000'\n",
      " '2004-08-23T00:00:00.000000000' '2004-08-25T00:00:00.000000000'\n",
      " '2004-08-27T00:00:00.000000000' '2004-08-29T00:00:00.000000000'\n",
      " '2004-08-31T00:00:00.000000000' '2004-09-02T00:00:00.000000000'\n",
      " '2004-09-04T00:00:00.000000000' '2004-09-06T00:00:00.000000000'\n",
      " '2004-09-08T00:00:00.000000000' '2004-09-10T00:00:00.000000000'\n",
      " '2004-09-12T00:00:00.000000000' '2004-09-14T00:00:00.000000000'\n",
      " '2004-09-16T00:00:00.000000000' '2004-09-18T00:00:00.000000000'\n",
      " '2004-09-20T00:00:00.000000000' '2004-09-22T00:00:00.000000000'\n",
      " '2004-09-24T00:00:00.000000000' '2004-09-26T00:00:00.000000000'\n",
      " '2004-09-28T00:00:00.000000000' '2004-09-30T00:00:00.000000000'\n",
      " '2004-10-02T00:00:00.000000000' '2004-10-04T00:00:00.000000000'\n",
      " '2004-10-06T00:00:00.000000000' '2004-10-08T00:00:00.000000000'\n",
      " '2004-10-10T00:00:00.000000000' '2004-10-12T00:00:00.000000000'\n",
      " '2004-10-14T00:00:00.000000000' '2004-10-16T00:00:00.000000000'\n",
      " '2004-10-18T00:00:00.000000000' '2004-10-20T00:00:00.000000000'\n",
      " '2004-10-22T00:00:00.000000000' '2004-10-24T00:00:00.000000000'\n",
      " '2004-10-26T00:00:00.000000000' '2004-10-28T00:00:00.000000000'\n",
      " '2004-10-30T00:00:00.000000000' '2004-11-01T00:00:00.000000000'\n",
      " '2004-11-03T00:00:00.000000000' '2004-11-05T00:00:00.000000000'\n",
      " '2004-11-07T00:00:00.000000000' '2004-11-09T00:00:00.000000000'\n",
      " '2004-11-11T00:00:00.000000000' '2004-11-13T00:00:00.000000000'\n",
      " '2004-11-15T00:00:00.000000000' '2004-11-17T00:00:00.000000000'\n",
      " '2004-11-19T00:00:00.000000000' '2004-11-21T00:00:00.000000000'\n",
      " '2004-11-23T00:00:00.000000000' '2004-11-25T00:00:00.000000000'\n",
      " '2004-11-27T00:00:00.000000000' '2004-11-29T00:00:00.000000000'\n",
      " '2004-12-01T00:00:00.000000000' '2004-12-03T00:00:00.000000000'\n",
      " '2004-12-05T00:00:00.000000000' '2004-12-07T00:00:00.000000000'\n",
      " '2004-12-09T00:00:00.000000000' '2004-12-11T00:00:00.000000000'\n",
      " '2004-12-13T00:00:00.000000000' '2004-12-15T00:00:00.000000000'\n",
      " '2004-12-17T00:00:00.000000000' '2004-12-19T00:00:00.000000000'\n",
      " '2004-12-21T00:00:00.000000000' '2004-12-23T00:00:00.000000000'\n",
      " '2004-12-25T00:00:00.000000000' '2004-12-27T00:00:00.000000000'\n",
      " '2004-12-29T00:00:00.000000000' '2004-12-31T00:00:00.000000000']\n",
      "['2002-12-28T00:00:00.000000000' '2002-12-30T00:00:00.000000000'\n",
      " '2003-01-01T00:00:00.000000000' '2003-01-03T00:00:00.000000000'\n",
      " '2003-01-05T00:00:00.000000000' '2003-01-07T00:00:00.000000000'\n",
      " '2003-01-09T00:00:00.000000000' '2003-01-11T00:00:00.000000000'\n",
      " '2003-01-13T00:00:00.000000000' '2003-01-15T00:00:00.000000000'\n",
      " '2003-01-17T00:00:00.000000000' '2003-01-19T00:00:00.000000000'\n",
      " '2003-01-21T00:00:00.000000000' '2003-01-23T00:00:00.000000000'\n",
      " '2003-01-25T00:00:00.000000000' '2003-01-27T00:00:00.000000000'\n",
      " '2003-01-29T00:00:00.000000000' '2003-01-31T00:00:00.000000000'\n",
      " '2003-02-02T00:00:00.000000000' '2003-02-04T00:00:00.000000000'\n",
      " '2003-02-06T00:00:00.000000000' '2003-02-08T00:00:00.000000000'\n",
      " '2003-02-10T00:00:00.000000000' '2003-02-12T00:00:00.000000000'\n",
      " '2003-02-14T00:00:00.000000000' '2003-02-16T00:00:00.000000000'\n",
      " '2003-02-18T00:00:00.000000000' '2003-02-20T00:00:00.000000000'\n",
      " '2003-02-22T00:00:00.000000000' '2003-02-24T00:00:00.000000000'\n",
      " '2003-02-26T00:00:00.000000000' '2003-02-28T00:00:00.000000000'\n",
      " '2003-03-02T00:00:00.000000000' '2003-03-04T00:00:00.000000000'\n",
      " '2003-03-06T00:00:00.000000000' '2003-03-08T00:00:00.000000000'\n",
      " '2003-03-10T00:00:00.000000000' '2003-03-12T00:00:00.000000000'\n",
      " '2003-03-14T00:00:00.000000000' '2003-03-16T00:00:00.000000000'\n",
      " '2003-03-18T00:00:00.000000000' '2003-03-20T00:00:00.000000000'\n",
      " '2003-03-22T00:00:00.000000000' '2003-03-24T00:00:00.000000000'\n",
      " '2003-03-26T00:00:00.000000000' '2003-03-28T00:00:00.000000000'\n",
      " '2003-03-30T00:00:00.000000000' '2003-04-01T00:00:00.000000000'\n",
      " '2003-04-03T00:00:00.000000000' '2003-04-05T00:00:00.000000000'\n",
      " '2003-04-07T00:00:00.000000000' '2003-04-09T00:00:00.000000000'\n",
      " '2003-04-11T00:00:00.000000000' '2003-04-13T00:00:00.000000000'\n",
      " '2003-04-15T00:00:00.000000000' '2003-04-17T00:00:00.000000000'\n",
      " '2003-04-19T00:00:00.000000000' '2003-04-21T00:00:00.000000000'\n",
      " '2003-04-23T00:00:00.000000000' '2003-04-25T00:00:00.000000000'\n",
      " '2003-04-27T00:00:00.000000000' '2003-04-29T00:00:00.000000000'\n",
      " '2003-05-01T00:00:00.000000000' '2003-05-03T00:00:00.000000000'\n",
      " '2003-05-05T00:00:00.000000000' '2003-05-07T00:00:00.000000000'\n",
      " '2003-05-09T00:00:00.000000000' '2003-05-11T00:00:00.000000000'\n",
      " '2003-05-13T00:00:00.000000000' '2003-05-15T00:00:00.000000000'\n",
      " '2003-05-17T00:00:00.000000000' '2003-05-19T00:00:00.000000000'\n",
      " '2003-05-21T00:00:00.000000000' '2003-05-23T00:00:00.000000000'\n",
      " '2003-05-25T00:00:00.000000000' '2003-05-27T00:00:00.000000000'\n",
      " '2003-05-29T00:00:00.000000000' '2003-05-31T00:00:00.000000000'\n",
      " '2003-06-02T00:00:00.000000000' '2003-06-04T00:00:00.000000000'\n",
      " '2003-06-06T00:00:00.000000000' '2003-06-08T00:00:00.000000000'\n",
      " '2003-06-10T00:00:00.000000000' '2003-06-12T00:00:00.000000000'\n",
      " '2003-06-14T00:00:00.000000000' '2003-06-16T00:00:00.000000000'\n",
      " '2003-06-18T00:00:00.000000000' '2003-06-20T00:00:00.000000000'\n",
      " '2003-06-22T00:00:00.000000000' '2003-06-24T00:00:00.000000000'\n",
      " '2003-06-26T00:00:00.000000000' '2003-06-28T00:00:00.000000000'\n",
      " '2003-06-30T00:00:00.000000000' '2003-07-02T00:00:00.000000000'\n",
      " '2003-07-04T00:00:00.000000000' '2003-07-06T00:00:00.000000000'\n",
      " '2003-07-08T00:00:00.000000000' '2003-07-10T00:00:00.000000000'\n",
      " '2003-07-12T00:00:00.000000000' '2003-07-14T00:00:00.000000000'\n",
      " '2003-07-16T00:00:00.000000000' '2003-07-18T00:00:00.000000000'\n",
      " '2003-07-20T00:00:00.000000000' '2003-07-22T00:00:00.000000000'\n",
      " '2003-07-24T00:00:00.000000000' '2003-07-26T00:00:00.000000000'\n",
      " '2003-07-28T00:00:00.000000000' '2003-07-30T00:00:00.000000000'\n",
      " '2003-08-01T00:00:00.000000000' '2003-08-03T00:00:00.000000000'\n",
      " '2003-08-05T00:00:00.000000000' '2003-08-07T00:00:00.000000000'\n",
      " '2003-08-09T00:00:00.000000000' '2003-08-11T00:00:00.000000000'\n",
      " '2003-08-13T00:00:00.000000000' '2003-08-15T00:00:00.000000000'\n",
      " '2003-08-17T00:00:00.000000000' '2003-08-19T00:00:00.000000000'\n",
      " '2003-08-21T00:00:00.000000000' '2003-08-23T00:00:00.000000000'\n",
      " '2003-08-25T00:00:00.000000000' '2003-08-27T00:00:00.000000000'\n",
      " '2003-08-29T00:00:00.000000000' '2003-08-31T00:00:00.000000000'\n",
      " '2003-09-02T00:00:00.000000000' '2003-09-04T00:00:00.000000000'\n",
      " '2003-09-06T00:00:00.000000000' '2003-09-08T00:00:00.000000000'\n",
      " '2003-09-10T00:00:00.000000000' '2003-09-12T00:00:00.000000000'\n",
      " '2003-09-14T00:00:00.000000000' '2003-09-16T00:00:00.000000000'\n",
      " '2003-09-18T00:00:00.000000000' '2003-09-20T00:00:00.000000000'\n",
      " '2003-09-22T00:00:00.000000000' '2003-09-24T00:00:00.000000000'\n",
      " '2003-09-26T00:00:00.000000000' '2003-09-28T00:00:00.000000000'\n",
      " '2003-09-30T00:00:00.000000000' '2003-10-02T00:00:00.000000000'\n",
      " '2003-10-04T00:00:00.000000000' '2003-10-06T00:00:00.000000000'\n",
      " '2003-10-08T00:00:00.000000000' '2003-10-10T00:00:00.000000000'\n",
      " '2003-10-12T00:00:00.000000000' '2003-10-14T00:00:00.000000000'\n",
      " '2003-10-16T00:00:00.000000000' '2003-10-18T00:00:00.000000000'\n",
      " '2003-10-20T00:00:00.000000000' '2003-10-22T00:00:00.000000000'\n",
      " '2003-10-24T00:00:00.000000000' '2003-10-26T00:00:00.000000000'\n",
      " '2003-10-28T00:00:00.000000000' '2003-10-30T00:00:00.000000000'\n",
      " '2003-11-01T00:00:00.000000000' '2003-11-03T00:00:00.000000000'\n",
      " '2003-11-05T00:00:00.000000000' '2003-11-07T00:00:00.000000000'\n",
      " '2003-11-09T00:00:00.000000000' '2003-11-11T00:00:00.000000000'\n",
      " '2003-11-13T00:00:00.000000000' '2003-11-15T00:00:00.000000000'\n",
      " '2003-11-17T00:00:00.000000000' '2003-11-19T00:00:00.000000000'\n",
      " '2003-11-21T00:00:00.000000000' '2003-11-23T00:00:00.000000000'\n",
      " '2003-11-25T00:00:00.000000000' '2003-11-27T00:00:00.000000000'\n",
      " '2003-11-29T00:00:00.000000000' '2003-12-01T00:00:00.000000000'\n",
      " '2003-12-03T00:00:00.000000000' '2003-12-05T00:00:00.000000000'\n",
      " '2003-12-07T00:00:00.000000000' '2003-12-09T00:00:00.000000000'\n",
      " '2003-12-11T00:00:00.000000000' '2003-12-13T00:00:00.000000000'\n",
      " '2003-12-15T00:00:00.000000000' '2003-12-17T00:00:00.000000000'\n",
      " '2003-12-19T00:00:00.000000000' '2003-12-21T00:00:00.000000000'\n",
      " '2003-12-23T00:00:00.000000000' '2003-12-25T00:00:00.000000000'\n",
      " '2003-12-27T00:00:00.000000000' '2003-12-29T00:00:00.000000000'\n",
      " '2003-12-31T00:00:00.000000000' '2004-01-02T00:00:00.000000000'\n",
      " '2004-01-04T00:00:00.000000000' '2004-01-06T00:00:00.000000000'\n",
      " '2004-01-08T00:00:00.000000000' '2004-01-10T00:00:00.000000000'\n",
      " '2004-01-12T00:00:00.000000000' '2004-01-14T00:00:00.000000000'\n",
      " '2004-01-16T00:00:00.000000000' '2004-01-18T00:00:00.000000000'\n",
      " '2004-01-20T00:00:00.000000000' '2004-01-22T00:00:00.000000000'\n",
      " '2004-01-24T00:00:00.000000000' '2004-01-26T00:00:00.000000000'\n",
      " '2004-01-28T00:00:00.000000000' '2004-01-30T00:00:00.000000000'\n",
      " '2004-02-01T00:00:00.000000000' '2004-02-03T00:00:00.000000000'\n",
      " '2004-02-05T00:00:00.000000000' '2004-02-07T00:00:00.000000000'\n",
      " '2004-02-09T00:00:00.000000000' '2004-02-11T00:00:00.000000000'\n",
      " '2004-02-13T00:00:00.000000000' '2004-02-15T00:00:00.000000000'\n",
      " '2004-02-17T00:00:00.000000000' '2004-02-19T00:00:00.000000000'\n",
      " '2004-02-21T00:00:00.000000000' '2004-02-23T00:00:00.000000000'\n",
      " '2004-02-25T00:00:00.000000000' '2004-02-27T00:00:00.000000000'\n",
      " '2004-02-29T00:00:00.000000000' '2004-03-02T00:00:00.000000000'\n",
      " '2004-03-04T00:00:00.000000000' '2004-03-06T00:00:00.000000000'\n",
      " '2004-03-08T00:00:00.000000000' '2004-03-10T00:00:00.000000000'\n",
      " '2004-03-12T00:00:00.000000000' '2004-03-14T00:00:00.000000000'\n",
      " '2004-03-16T00:00:00.000000000' '2004-03-18T00:00:00.000000000'\n",
      " '2004-03-20T00:00:00.000000000' '2004-03-22T00:00:00.000000000'\n",
      " '2004-03-24T00:00:00.000000000' '2004-03-26T00:00:00.000000000'\n",
      " '2004-03-28T00:00:00.000000000' '2004-03-30T00:00:00.000000000'\n",
      " '2004-04-01T00:00:00.000000000' '2004-04-03T00:00:00.000000000'\n",
      " '2004-04-05T00:00:00.000000000' '2004-04-07T00:00:00.000000000'\n",
      " '2004-04-09T00:00:00.000000000' '2004-04-11T00:00:00.000000000'\n",
      " '2004-04-13T00:00:00.000000000' '2004-04-15T00:00:00.000000000'\n",
      " '2004-04-17T00:00:00.000000000' '2004-04-19T00:00:00.000000000'\n",
      " '2004-04-21T00:00:00.000000000' '2004-04-23T00:00:00.000000000'\n",
      " '2004-04-25T00:00:00.000000000' '2004-04-27T00:00:00.000000000'\n",
      " '2004-04-29T00:00:00.000000000' '2004-05-01T00:00:00.000000000'\n",
      " '2004-05-03T00:00:00.000000000' '2004-05-05T00:00:00.000000000'\n",
      " '2004-05-07T00:00:00.000000000' '2004-05-09T00:00:00.000000000'\n",
      " '2004-05-11T00:00:00.000000000' '2004-05-13T00:00:00.000000000'\n",
      " '2004-05-15T00:00:00.000000000' '2004-05-17T00:00:00.000000000'\n",
      " '2004-05-19T00:00:00.000000000' '2004-05-21T00:00:00.000000000'\n",
      " '2004-05-23T00:00:00.000000000' '2004-05-25T00:00:00.000000000'\n",
      " '2004-05-27T00:00:00.000000000' '2004-05-29T00:00:00.000000000'\n",
      " '2004-05-31T00:00:00.000000000' '2004-06-02T00:00:00.000000000'\n",
      " '2004-06-04T00:00:00.000000000' '2004-06-06T00:00:00.000000000'\n",
      " '2004-06-08T00:00:00.000000000' '2004-06-10T00:00:00.000000000'\n",
      " '2004-06-12T00:00:00.000000000' '2004-06-14T00:00:00.000000000'\n",
      " '2004-06-16T00:00:00.000000000' '2004-06-18T00:00:00.000000000'\n",
      " '2004-06-20T00:00:00.000000000' '2004-06-22T00:00:00.000000000'\n",
      " '2004-06-24T00:00:00.000000000' '2004-06-26T00:00:00.000000000'\n",
      " '2004-06-28T00:00:00.000000000' '2004-06-30T00:00:00.000000000'\n",
      " '2004-07-02T00:00:00.000000000' '2004-07-04T00:00:00.000000000'\n",
      " '2004-07-06T00:00:00.000000000' '2004-07-08T00:00:00.000000000'\n",
      " '2004-07-10T00:00:00.000000000' '2004-07-12T00:00:00.000000000'\n",
      " '2004-07-14T00:00:00.000000000' '2004-07-16T00:00:00.000000000'\n",
      " '2004-07-18T00:00:00.000000000' '2004-07-20T00:00:00.000000000'\n",
      " '2004-07-22T00:00:00.000000000' '2004-07-24T00:00:00.000000000'\n",
      " '2004-07-26T00:00:00.000000000' '2004-07-28T00:00:00.000000000'\n",
      " '2004-07-30T00:00:00.000000000' '2004-08-01T00:00:00.000000000'\n",
      " '2004-08-03T00:00:00.000000000' '2004-08-05T00:00:00.000000000'\n",
      " '2004-08-07T00:00:00.000000000' '2004-08-09T00:00:00.000000000'\n",
      " '2004-08-11T00:00:00.000000000' '2004-08-13T00:00:00.000000000'\n",
      " '2004-08-15T00:00:00.000000000' '2004-08-17T00:00:00.000000000'\n",
      " '2004-08-19T00:00:00.000000000' '2004-08-21T00:00:00.000000000'\n",
      " '2004-08-23T00:00:00.000000000' '2004-08-25T00:00:00.000000000'\n",
      " '2004-08-27T00:00:00.000000000' '2004-08-29T00:00:00.000000000'\n",
      " '2004-08-31T00:00:00.000000000' '2004-09-02T00:00:00.000000000'\n",
      " '2004-09-04T00:00:00.000000000' '2004-09-06T00:00:00.000000000'\n",
      " '2004-09-08T00:00:00.000000000' '2004-09-10T00:00:00.000000000'\n",
      " '2004-09-12T00:00:00.000000000' '2004-09-14T00:00:00.000000000'\n",
      " '2004-09-16T00:00:00.000000000' '2004-09-18T00:00:00.000000000'\n",
      " '2004-09-20T00:00:00.000000000' '2004-09-22T00:00:00.000000000'\n",
      " '2004-09-24T00:00:00.000000000' '2004-09-26T00:00:00.000000000'\n",
      " '2004-09-28T00:00:00.000000000' '2004-09-30T00:00:00.000000000'\n",
      " '2004-10-02T00:00:00.000000000' '2004-10-04T00:00:00.000000000'\n",
      " '2004-10-06T00:00:00.000000000' '2004-10-08T00:00:00.000000000'\n",
      " '2004-10-10T00:00:00.000000000' '2004-10-12T00:00:00.000000000'\n",
      " '2004-10-14T00:00:00.000000000' '2004-10-16T00:00:00.000000000'\n",
      " '2004-10-18T00:00:00.000000000' '2004-10-20T00:00:00.000000000'\n",
      " '2004-10-22T00:00:00.000000000' '2004-10-24T00:00:00.000000000'\n",
      " '2004-10-26T00:00:00.000000000' '2004-10-28T00:00:00.000000000'\n",
      " '2004-10-30T00:00:00.000000000' '2004-11-01T00:00:00.000000000'\n",
      " '2004-11-03T00:00:00.000000000' '2004-11-05T00:00:00.000000000'\n",
      " '2004-11-07T00:00:00.000000000' '2004-11-09T00:00:00.000000000'\n",
      " '2004-11-11T00:00:00.000000000' '2004-11-13T00:00:00.000000000'\n",
      " '2004-11-15T00:00:00.000000000' '2004-11-17T00:00:00.000000000'\n",
      " '2004-11-19T00:00:00.000000000' '2004-11-21T00:00:00.000000000'\n",
      " '2004-11-23T00:00:00.000000000' '2004-11-25T00:00:00.000000000'\n",
      " '2004-11-27T00:00:00.000000000' '2004-11-29T00:00:00.000000000'\n",
      " '2004-12-01T00:00:00.000000000' '2004-12-03T00:00:00.000000000'\n",
      " '2004-12-05T00:00:00.000000000' '2004-12-07T00:00:00.000000000'\n",
      " '2004-12-09T00:00:00.000000000' '2004-12-11T00:00:00.000000000'\n",
      " '2004-12-13T00:00:00.000000000' '2004-12-15T00:00:00.000000000'\n",
      " '2004-12-17T00:00:00.000000000' '2004-12-19T00:00:00.000000000'\n",
      " '2004-12-21T00:00:00.000000000' '2004-12-23T00:00:00.000000000'\n",
      " '2004-12-25T00:00:00.000000000' '2004-12-27T00:00:00.000000000'\n",
      " '2004-12-29T00:00:00.000000000' '2004-12-31T00:00:00.000000000'\n",
      " '2005-01-02T00:00:00.000000000' '2005-01-04T00:00:00.000000000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/common.py:594: FutureWarning: pd.TimeGrouper is deprecated and will be removed; Please use pd.Grouper(freq=...)\n",
      "  label=label, base=base)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Boolean array size 146 is used to index array with shape (147,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-335-19cea245a57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Only keep resample bins that are complete -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mdata_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;31m# xarray-style array indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_key_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36misel\u001b[0;34m(self, drop, **indexers)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mDataArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_temp_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_temp_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36misel\u001b[0;34m(self, drop, **indexers)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0mvar_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexers_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m             \u001b[0mnew_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvar_indexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_indexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36misel\u001b[0;34m(self, **indexers)\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m         \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broadcast_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m_broadcast_indexes\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broadcast_indexes_basic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0;31m# Detect it can be mapped as an outer indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# If all key is unlabeled, or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m_validate_indexers\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    520\u001b[0m                             \u001b[0;34m\"Boolean array size {0:d} is used to index array \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                             \"with shape {1:s}.\".format(len(k),\n\u001b[0;32m--> 522\u001b[0;31m                                                        str(self.shape)))\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                         raise IndexError(\"{}-dimensional boolean indexing is \"\n",
      "\u001b[0;31mIndexError\u001b[0m: Boolean array size 146 is used to index array with shape (147,)."
     ]
    }
   ],
   "source": [
    "temp = da_fcst.isel(init_date=[0],lead_time=range(0,731,2))\n",
    "temp.lead_time.attrs['units'] = '2D'\n",
    "\n",
    "data = leadtime_to_datetime(temp)\n",
    "resample_freq='5D'\n",
    "how='sum'\n",
    "input_freq = None\n",
    "\n",
    "#def downsample_complete(data, resample_freq, how, input_freq=None):\n",
    "\n",
    "dates = data.time.values\n",
    "\n",
    "# Try to infer input frequency -----\n",
    "if input_freq == None:\n",
    "    input_freq = utils.infer_freq(dates)\n",
    "if input_freq == None:\n",
    "    raise ValueError('Unable to infer input frequency. Please specify this explicity.')\n",
    "\n",
    "# Split frequencies into numbers and strings -----\n",
    "incr_string = ''.join([i for i in resample_freq if i.isdigit()])\n",
    "resample_incr = [int(incr_string) if incr_string else 1][0]\n",
    "resample_type = ''.join([i for i in resample_freq if not i.isdigit()])\n",
    "\n",
    "incr_string = ''.join([i for i in input_freq if  i.isdigit()])\n",
    "input_incr = [int(incr_string) if incr_string else 1][0]\n",
    "input_type = ''.join([i for i in input_freq if not i.isdigit()])\n",
    "\n",
    "# Construct dummy date array to determine complete number of increments in each resample bin -----\n",
    "if 'M' in resample_type: # Deal with special case of months\n",
    "    start = month_delta(dates[0],-resample_incr)\n",
    "    end = month_delta(dates[-1],resample_incr)\n",
    "    \n",
    "    # Ensure dummy_dates align with dates in overlap region -----\n",
    "    left_chunk = (pd.date_range(start, dates[0], freq = input_freq)).values\n",
    "    left_chunk_aligned = left_chunk + (dates[0] - left_chunk[-1])\n",
    "    right_chunk_aligned = (pd.date_range(dates[-1], end, freq = input_freq)).values\n",
    "    dummy_dates = np.concatenate([left_chunk_aligned, dates[1:-1], right_chunk_aligned])\n",
    "    \n",
    "elif ('A' in resample_type) | ('Y' in resample_type): # Deal with special case of years\n",
    "    start = year_delta(dates[0],-resample_incr)\n",
    "    end = year_delta(dates[-1],resample_incr)\n",
    "    \n",
    "    # Ensure dummy_dates align with dates in overlap region -----\n",
    "    left_chunk = (pd.date_range(start, dates[0], freq = input_freq)).values\n",
    "    left_chunk_aligned = left_chunk + (dates[0] - left_chunk[-1])\n",
    "    right_chunk_aligned = (pd.date_range(dates[-1], end, freq = input_freq)).values\n",
    "    dummy_dates = np.concatenate([left_chunk_aligned, dates[1:-1], right_chunk_aligned])\n",
    "    \n",
    "else:\n",
    "    start = dates[0] - pd.Timedelta(resample_incr, unit = resample_type)\n",
    "    end = dates[-1] + pd.Timedelta(resample_incr, unit = resample_type)\n",
    "    \n",
    "    # Ensure dummy_dates align with dates in overlap region -----\n",
    "    left_chunk = (pd.date_range(start, dates[0], freq = input_freq)).values\n",
    "    left_chunk_aligned = left_chunk + (dates[0] - left_chunk[-1])\n",
    "    right_chunk_aligned = (pd.date_range(dates[-1], end, freq = input_freq)).values\n",
    "    dummy_dates = np.concatenate([left_chunk_aligned, dates[1:-1], right_chunk_aligned])\n",
    "    print(dates)\n",
    "    print(dummy_dates)\n",
    "    \n",
    "# Package dummy date array as xarray object and resample -----\n",
    "dummy = xr.DataArray(np.zeros(dummy_dates.shape), coords=[dummy_dates], dims='time')\n",
    "dummy_sampled = dummy.resample(time=resample_freq)\n",
    "data_sampled = data.resample(time=resample_freq)\n",
    "\n",
    "# Find and compare number of increments in each dummy bin and data bin -----\n",
    "dummy_incr = [len(dummy_bin.time) for name, dummy_bin in dummy_sampled][1:-1]\n",
    "data_incr = [len(data_bin.time) for name, data_bin in data_sampled]\n",
    "data_bins = [name for name, data_bin in data_sampled]\n",
    "keep = [dum == dat for (dum, dat) in zip(dummy_incr, data_incr)]\n",
    "print(dummy_incr)\n",
    "print(data_incr)\n",
    "\n",
    "# Perform resampling according to specified method -----\n",
    "if how == 'mean':\n",
    "    data_resampled = data_sampled.mean(dim='time',keep_attrs=True)\n",
    "elif how == 'sum':\n",
    "    data_resampled = data_sampled.sum(dim='time',keep_attrs=True)\n",
    "else:\n",
    "    raise ValueError(f'Unrecognised \"how\" method: {how}. Please feel free to add methods.')\n",
    "\n",
    "# Strangely, xarray.resample().how() adds an additional time step to the beginning or end of \n",
    "# the data (depending on whether the resample frequency is a start or end frequency) when the \n",
    "# time interval of the data being resampled is wholly divisible by the resampling frequency.\n",
    "# Data at this time step are all nans. Let's make sure we only keep output time steps that \n",
    "# exist in the xarray.core.resample.DataArrayResample object -----\n",
    "data_resampled = data_resampled.sel(time=slice(str(data_bins[0]), str(data_bins[-1])))\n",
    "\n",
    "# Only keep resample bins that are complete -----\n",
    "data_resampled = data_resampled.sel(time=data_resampled.time[keep])\n",
    "\n",
    "print(data_resampled)\n",
    "print(keep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = data_resampled.isel(time=[0], pfull = [-1], ensemble = [0]).squeeze().compute()\n",
    "last = data_resampled.isel(time=[12], pfull = [-1], ensemble = [0]).squeeze().compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'temp' (lat: 90, lon: 144)>\n",
       "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       ..., \n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan]])\n",
       "Coordinates:\n",
       "    time      datetime64[ns] 2005-01-01\n",
       "  * lon       (lon) float64 1.25 3.75 6.25 8.75 11.25 13.75 16.25 18.75 ...\n",
       "  * lat       (lat) float64 -89.49 -87.98 -85.96 -83.93 -81.91 -79.89 -77.87 ...\n",
       "    pfull     float64 996.1\n",
       "    ensemble  int64 1\n",
       "Attributes:\n",
       "    long_name:      temperature\n",
       "    units:          deg_K\n",
       "    valid_range:    [ 100.  350.]\n",
       "    cell_methods:   time: mean\n",
       "    time_avg_info:  average_T1,average_T2,average_DT"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2000-01-01T00:00:00.000000000', '2000-01-03T00:00:00.000000000',\n",
       "       '2000-01-05T00:00:00.000000000', '2000-01-07T00:00:00.000000000',\n",
       "       '2000-01-09T00:00:00.000000000', '2000-01-11T00:00:00.000000000',\n",
       "       '2000-01-13T00:00:00.000000000', '2000-01-15T00:00:00.000000000',\n",
       "       '2000-01-17T00:00:00.000000000', '2000-01-19T00:00:00.000000000',\n",
       "       '2000-01-21T00:00:00.000000000', '2000-01-23T00:00:00.000000000',\n",
       "       '2000-01-25T00:00:00.000000000', '2000-01-27T00:00:00.000000000',\n",
       "       '2000-01-29T00:00:00.000000000', '2000-01-31T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.date_range(start='1/1/2000', end='2/1/2000', freq = '2D', closed = 'left').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp = lambda data, freq, how: utils.datetime_to_leadtime(\n",
    "                                   resample(\n",
    "                                       utils.leadtime_to_datetime(data), freq, how))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for probabilistic forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for temperature at 1000hPa averaged over Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Region of interest -----\n",
    "    region = (-38.0, -11.0, 113.0 , 153.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "    da_fcst = utils.calc_boxavg_latlon(ds_fcst['temp']\n",
    "                                       .sel(pfull=1000,method='nearest',drop=True), region).compute()-273.15\n",
    "    da_obsv = utils.calc_boxavg_latlon(ds_obsv['temp']\n",
    "                                       .squeeze().drop('lv_ISBL1'), region).compute()-273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot one initialization date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(da_fcst['lead_time'],da_fcst.isel(init_date=[0]).squeeze())\n",
    "ax.plot(da_obsv['lead_time'],da_obsv.isel(init_date=[0]).squeeze(),'k-',linewidth=2)\n",
    "ax.set_xlabel('lead time')\n",
    "ax.set_ylabel('average temp [K]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank the data and compute histograms as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    rank_histogram = skill.compute_rank_histogram(da_fcst, da_obsv, indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(lead_times)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        ax.bar(rank_histogram.bins,rank_histogram.isel(lead_time=idx, drop=True))\n",
    "        ax.set_ylim(0,rank_histogram.max())\n",
    "        ax.text(10.3,0.85*rank_histogram.max(),'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('count')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank the data and compute histograms for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    rank_histogram = skill.compute_rank_histogram(da_fcst, da_obsv, indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,3))\n",
    "\n",
    "    ax1 = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax1.grid()\n",
    "    ax1.bar(rank_histogram.bins,rank_histogram)\n",
    "    ax1.set_xlabel('bins')\n",
    "    ax1.set_ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Continuous) ranked probability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Specify bins for computation of cdf -----\n",
    "    bins = np.linspace(0,40,100)\n",
    "\n",
    "    # Compute ranked probability score -----\n",
    "    rps = skill.compute_rps(da_fcst, da_obsv, bins=bins, indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(rps['lead_time'],rps,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Ranked probability score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive bias error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_additive_bias = skill.compute_mean_additive_bias(da_fcst, da_obsv, \n",
    "                                                          indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicative bias error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_multiplicative_bias = skill.compute_mean_multiplicative_bias(da_fcst, da_obsv, \n",
    "                                                                      indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_absolute_error = skill.compute_mean_absolute_error(da_fcst, da_obsv, \n",
    "                                                            indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_squared_error = skill.compute_mean_squared_error(da_fcst, da_obsv, \n",
    "                                                          indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    rms_error = skill.compute_rms_error(da_fcst, da_obsv, \n",
    "                                        indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(mean_additive_bias['lead_time'],mean_additive_bias,linewidth=2)\n",
    "    ax.plot(mean_multiplicative_bias['lead_time'],mean_multiplicative_bias,linewidth=2)\n",
    "    ax.plot(mean_absolute_error['lead_time'],mean_absolute_error,linewidth=2)\n",
    "    # ax.plot(mean_squared_error['lead_time'],mean_squared_error,linewidth=2)\n",
    "    ax.plot(rms_error['lead_time'],rms_error,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Error');\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with utils.timer():\n",
    "#     client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dts3_env]",
   "language": "python",
   "name": "conda-env-dts3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
